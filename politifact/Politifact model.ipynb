{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO - Clean and run naive bayes and pac\\n         2 models, 1 with URL and other without URL\\n         REQUIRES A LOT OF COMMENTING AND DOCUMENTATION\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO - Clean and run naive bayes and pac\n",
    "         2 models, 1 with URL and other without URL\n",
    "         REQUIRES A LOT OF COMMENTING AND DOCUMENTATION\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from auto_naive.ipynb\n",
      "importing Jupyter notebook from auto_pac.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "import auto_naive as an\n",
    "import auto_pac as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 4022: expected 6 fields, saw 7\\nSkipping line 6316: expected 6 fields, saw 7\\nSkipping line 9381: expected 6 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "#Reading this for extraction of URL\n",
    "pol_data = pd.read_csv('datasets/politifact_data/full_politifact_data.csv',sep='\\t',error_bad_lines=False,index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10464, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_data.head()\n",
    "#Politifact explanations are not required\n",
    "pol_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_data.URL.to_csv('datasets/politifact_data/pol_data.URL',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning and extracting most important part of url\n",
    "re_url=pd.read_csv('datasets/politifact_data/pol_data.URL',names=['id','url'])\n",
    "\n",
    "url_only=re_url.url\n",
    "e_url=[0]*url_only.size\n",
    "\n",
    "for i in range(url_only.size):\n",
    "    e_url[i]=url_only[i].split(\"/\")\n",
    "\n",
    "clean_url=[0]*url_only.size\n",
    "\n",
    "for i in range(url_only.size):\n",
    "    clean_url[i]=e_url[i][len(e_url[i])-2]\n",
    "\n",
    "del clean_url[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Speaker', 'Truth-Rating', 'Statement', 'PolitiFact explanations',\n",
       "       'URL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Speaker', 'Truth-Rating', 'Statement', 'PolitiFact explanations'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#dropping old url and updating with clean\n",
    "pol_data.drop(['URL'], axis=1,inplace=True)\n",
    "print(pol_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10464, 4)\n",
      "10464\n"
     ]
    }
   ],
   "source": [
    "print(pol_data.shape)\n",
    "print(len(clean_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_data.insert(loc=2, column='url', value=clean_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_data.drop(['PolitiFact explanations'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Speaker  Truth-Rating  \\\n",
      "0         Paul LePage             5   \n",
      "1  Battleground Texas             0   \n",
      "2  Battleground Texas             1   \n",
      "3      Suzanne Somers             0   \n",
      "4        Jamie Oliver             2   \n",
      "\n",
      "                                                 url  \\\n",
      "0  maine-gov-paul-lepage-says-47-percent-able-bod...   \n",
      "1  dan-patrick-has-called-illegal-immigration-inv...   \n",
      "2  battleground-texas-says-54-percent-texas-latin...   \n",
      "3  suzanne-somers-says-even-obamacare-tens-millio...   \n",
      "4        chef-jamie-oliver-praises-mcdonalds-england   \n",
      "\n",
      "                                           Statement  \n",
      "0  About 47 percent of able-bodied people in the ...  \n",
      "1  Says Dan Patrick has \"called immigration into ...  \n",
      "2  In 2008, \"only 54 percent of Latinos in Texas ...  \n",
      "3  Even after Obamacare is fully implemented, the...  \n",
      "4  McDonald's in England only sells organic milk ...  \n"
     ]
    }
   ],
   "source": [
    "pol_data.columns\n",
    "print(pol_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10464, 4)\n"
     ]
    }
   ],
   "source": [
    "#splitting target attribute from the rest\n",
    "\n",
    "type(pol_data)\n",
    "pol_data.rename(columns={\"Truth-Rating\": \"label\"},inplace=True)\n",
    "print(pol_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10464, 3)\n",
      "(10464,)\n",
      "0    5\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    2\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pol_data_y=pol_data.label\n",
    "pol_data_x=pol_data.drop(['label'],axis=1)\n",
    "print(pol_data_x.shape)\n",
    "print(pol_data_y.shape)\n",
    "\n",
    "print(pol_data_y.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2     True\n",
       "3     True\n",
       "4     True\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Former Ratings [ 0:True 1:Mostly True 2:Half True 3:Half false 4:Mostly false 5:False]\n",
    "#Actual Ratings [0,1,2 = 1 and 3,4,5 = 0]\\\n",
    "\n",
    "pol_data_y.replace({0:\"True\",1:\"True\",2:\"True\",3:\"False\",4:\"False\",5:\"False\"},inplace=True)\n",
    "\n",
    "pol_data_y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNaive Bayes Models : \\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pol_data_x.columns\n",
    "\"\"\"\n",
    "Naive Bayes Models : \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "train_x : (8371,)\n",
      "train_y : (8371,)\n",
      "test_x : (2093,)\n",
      "test_y : (2093,)\n",
      "Vectorizing\n",
      "test_x : (2093,)\n",
      "train_x : (8371,)\n",
      "tfidf_train_x :  (8371, 10646)\n",
      "tfidf_test_x :  (2093, 10646)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.6093656671843268\n",
      "grid alpha 0.463\n",
      "Alpha: 0.463 test_Score: 0.60153 train_score: 0.82834\n",
      "training confusion matrix\n",
      "[[2428 1214]\n",
      " [ 223 4506]]\n",
      "testing confusion matrix\n",
      "[[290 609]\n",
      " [225 969]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_tfidf_model(pol_data_x.Statement,pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "train_x : (8371,)\n",
      "train_y : (8371,)\n",
      "test_x : (2093,)\n",
      "test_y : (2093,)\n",
      "Vectorizing\n",
      "test_x : (2093,)\n",
      "train_x : (8371,)\n",
      "tfidf_train_x :  (8371, 2879)\n",
      "tfidf_test_x :  (2093, 2879)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.5951499223509736\n",
      "grid alpha 0.985\n",
      "Alpha: 0.985 test_Score: 0.60248 train_score: 0.72369\n",
      "training confusion matrix\n",
      "[[1999 1643]\n",
      " [ 670 4059]]\n",
      "testing confusion matrix\n",
      "[[316 583]\n",
      " [249 945]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_tfidf_model(pol_data_x.Speaker,pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "train_x : (8371,)\n",
      "train_y : (8371,)\n",
      "test_x : (2093,)\n",
      "test_y : (2093,)\n",
      "Vectorizing\n",
      "test_x : (2093,)\n",
      "train_x : (8371,)\n",
      "tfidf_train_x :  (8371, 9785)\n",
      "tfidf_test_x :  (2093, 9785)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.6038705053159719\n",
      "grid alpha 0.732\n",
      "Alpha: 0.732 test_Score: 0.61921 train_score: 0.84339\n",
      "training confusion matrix\n",
      "[[2557 1085]\n",
      " [ 226 4503]]\n",
      "testing confusion matrix\n",
      "[[318 581]\n",
      " [216 978]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_tfidf_model(pol_data_x.url,pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Vectorizing\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.6014813045036436\n",
      "grid alpha 0.011\n",
      "Alpha: 0.011 test_Score: 0.59866 train_score: 0.84207\n",
      "training confusion matrix\n",
      "[[2411 1231]\n",
      " [  91 4638]]\n",
      "testing confusion matrix\n",
      "[[277 622]\n",
      " [218 976]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_hash_model(pol_data_x.Statement,pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Vectorizing\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.5956277625134393\n",
      "grid alpha 0.14\n",
      "Alpha: 0.140 test_Score: 0.60057 train_score: 0.80456\n",
      "training confusion matrix\n",
      "[[2057 1585]\n",
      " [  51 4678]]\n",
      "testing confusion matrix\n",
      "[[ 191  708]\n",
      " [ 128 1066]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_hash_model(pol_data_x.url,pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Vectorizing\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.5988531836100824\n",
      "grid alpha 0.426\n",
      "Alpha: 0.426 test_Score: 0.61108 train_score: 0.71258\n",
      "training confusion matrix\n",
      "[[1681 1961]\n",
      " [ 445 4284]]\n",
      "testing confusion matrix\n",
      "[[ 266  633]\n",
      " [ 181 1013]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_hash_model(pol_data_x.Speaker,pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_tfidf(x1,x2,y,x3=a):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    if x3.size==0:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "\n",
    "\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7)   # a TFIDF vectorizer\n",
    "        tfidf_train_x1 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x1).todense())  #fitting the training data\n",
    "        tfidf_test_x1 = pd.DataFrame(tfidf_vectorizer.transform(test_x1).todense())\n",
    "\n",
    "        tfidf_train_x2 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x2).todense())  #fitting the training data\n",
    "        tfidf_test_x2 = pd.DataFrame(tfidf_vectorizer.transform(test_x2).todense()) \n",
    "\n",
    "        train=pd.concat([tfidf_train_x1,tfidf_train_x2],axis=1)\n",
    "        test=pd.concat([tfidf_test_x1,tfidf_test_x2],axis=1)\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "        print(train_y2.shape)\n",
    "        print(test_y2.shape)\n",
    "\n",
    "        an.navie_bayes(train,train_y2,test,test_y2)\n",
    "    \n",
    "    else:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7)   # a TFIDF vectorizer\n",
    "        tfidf_train_x1 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x1).todense())  #fitting the training data\n",
    "        tfidf_test_x1 = pd.DataFrame(tfidf_vectorizer.transform(test_x1).todense())\n",
    "\n",
    "        tfidf_train_x2 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x2).todense())  #fitting the training data\n",
    "        tfidf_test_x2 = pd.DataFrame(tfidf_vectorizer.transform(test_x2).todense()) \n",
    "        \n",
    "        tfidf_train_x3 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x3).todense())  #fitting the training data\n",
    "        tfidf_test_x3 = pd.DataFrame(tfidf_vectorizer.transform(test_x3).todense())\n",
    "        \n",
    "        train=pd.concat([tfidf_train_x1,tfidf_train_x2,tfidf_train_x3],axis=1)\n",
    "        test=pd.concat([tfidf_test_x1,tfidf_test_x2,tfidf_test_x3],axis=1)\n",
    "        \n",
    "        an.navie_bayes(train,train_y2,test,test_y2)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 12664)\n",
      "(2093, 12664)\n",
      "(8371,)\n",
      "(2093,)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.6100824274280253\n",
      "grid alpha 0.999\n",
      "Alpha: 0.999 test_Score: 0.63641 train_score: 0.81747\n",
      "training confusion matrix\n",
      "[[2554 1088]\n",
      " [ 440 4289]]\n",
      "testing confusion matrix\n",
      "[[363 536]\n",
      " [225 969]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mult_tfidf(pol_data_x.url,pol_data_x.Speaker,pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 20431)\n",
      "(2093, 20431)\n",
      "(8371,)\n",
      "(2093,)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.620953291124119\n",
      "grid alpha 0.9530000000000001\n",
      "Alpha: 0.953 test_Score: 0.62828 train_score: 0.86262\n",
      "training confusion matrix\n",
      "[[2639 1003]\n",
      " [ 147 4582]]\n",
      "testing confusion matrix\n",
      "[[ 313  586]\n",
      " [ 192 1002]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf(x1=pol_data_x.url,x2=pol_data_x.Statement,y=pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 13525)\n",
      "(2093, 13525)\n",
      "(8371,)\n",
      "(2093,)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.6208338310835025\n",
      "grid alpha 0.997\n",
      "Alpha: 0.997 test_Score: 0.64071 train_score: 0.81125\n",
      "training confusion matrix\n",
      "[[2415 1227]\n",
      " [ 353 4376]]\n",
      "testing confusion matrix\n",
      "[[ 335  564]\n",
      " [ 188 1006]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf(x1=pol_data_x.Statement,x2=pol_data_x.Speaker,y=pol_data_y,x3=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.6287181937641859\n",
      "grid alpha 0.9460000000000001\n",
      "Alpha: 0.946 test_Score: 0.63832 train_score: 0.85533\n",
      "training confusion matrix\n",
      "[[2706  936]\n",
      " [ 275 4454]]\n",
      "testing confusion matrix\n",
      "[[354 545]\n",
      " [212 982]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf(x1=pol_data_x.Statement,x2=pol_data_x.Speaker,y=pol_data_y,x3=pol_data_x.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([])\n",
    "a.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hashv(x1,x2,y,x3=a):\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    # Not sure about this, collisions may occur as I forced absolute cause naivie bayes is a bitch and n_features over \n",
    "                                                                                        #2**15 causes memory error\n",
    "    if x3.size==0:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "\n",
    "\n",
    "\n",
    "        hash_vectorizer = HashingVectorizer(stop_words='english',n_features = 2**15)   # a HASH vectorizer\n",
    "        hash_train_x1 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x1).todense()))  #fitting the training data\n",
    "        hash_test_x1 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x1).todense()))\n",
    "\n",
    "        hash_train_x2 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x2).todense()))  #fitting the training data\n",
    "        hash_test_x2 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x2).todense())) \n",
    "\n",
    "        train=pd.concat([hash_train_x1,hash_train_x2],axis=1)\n",
    "        test=pd.concat([hash_test_x1,hash_test_x2],axis=1)\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "        print(train_y2.shape)\n",
    "        print(test_y2.shape)\n",
    "\n",
    "        an.navie_bayes(train,train_y2,test,test_y2,100)\n",
    "    \n",
    "    else:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        \n",
    "        hash_vectorizer = HashingVectorizer(stop_words='english',n_features = 2**15)   # a HASH vectorizer\n",
    "        hash_train_x1 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x1).todense()) ) #fitting the training data\n",
    "        hash_test_x1 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x1).todense()))\n",
    "\n",
    "        hash_train_x2 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x2).todense()) ) #fitting the training data\n",
    "        hash_test_x2 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x2).todense()) )\n",
    "        \n",
    "        hash_train_x3 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x3).todense()))  #fitting the training data\n",
    "        hash_test_x3 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x3).todense()))\n",
    "        \n",
    "        train=pd.concat([hash_train_x1,hash_train_x2,hash_train_x3],axis=1)\n",
    "        test=pd.concat([hash_test_x1,hash_test_x2,hash_test_x3],axis=1)\n",
    "        \n",
    "        an.navie_bayes(train,train_y2,test,test_y2,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 65536)\n",
      "(2093, 65536)\n",
      "(8371,)\n",
      "(2093,)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.6214311312865847\n",
      "grid alpha 0.27\n",
      "Alpha: 0.270 test_Score: 0.62828 train_score: 0.80528\n",
      "training confusion matrix\n",
      "[[2268 1374]\n",
      " [ 256 4473]]\n",
      "testing confusion matrix\n",
      "[[ 298  601]\n",
      " [ 177 1017]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "multi_hashv(x1=pol_data_x.Statement,x2=pol_data_x.Speaker,y=pol_data_y,x3=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 65536)\n",
      "(2093, 65536)\n",
      "(8371,)\n",
      "(2093,)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.6184446302711742\n",
      "grid alpha 0.18000000000000002\n",
      "Alpha: 0.180 test_Score: 0.61586 train_score: 0.87469\n",
      "training confusion matrix\n",
      "[[2719  923]\n",
      " [ 126 4603]]\n",
      "testing confusion matrix\n",
      "[[327 572]\n",
      " [232 962]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "multi_hashv(x1=pol_data_x.Statement,x2=pol_data_x.url,y=pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 65536)\n",
      "(2093, 65536)\n",
      "(8371,)\n",
      "(2093,)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.610560267590491\n",
      "grid alpha 0.92\n",
      "Alpha: 0.920 test_Score: 0.63067 train_score: 0.75367\n",
      "training confusion matrix\n",
      "[[1823 1819]\n",
      " [ 243 4486]]\n",
      "testing confusion matrix\n",
      "[[ 249  650]\n",
      " [ 123 1071]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "multi_hashv(x1=pol_data_x.url,x2=pol_data_x.Speaker,y=pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.6302711742921993\n",
      "grid alpha 0.2\n",
      "Alpha: 0.200 test_Score: 0.63163 train_score: 0.86501\n",
      "training confusion matrix\n",
      "[[2706  936]\n",
      " [ 194 4535]]\n",
      "testing confusion matrix\n",
      "[[344 555]\n",
      " [216 978]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "multi_hashv(x1=pol_data_x.url,x2=pol_data_x.Speaker,y=pol_data_y,x3=pol_data_x.Statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n-------------------------------------------------------------------------\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start building pac models \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "train accuracy: 0.9941464580097957\n",
      "train confusion matrix\n",
      "[[3608   34]\n",
      " [  15 4714]]\n",
      "test accuracy: 0.5456282847587195\n",
      "test confusion matrix\n",
      "[[423 476]\n",
      " [475 719]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(pol_data_x.Statement,pol_data_y) #too much overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "train accuracy: 0.9948632182534942\n",
      "train confusion matrix\n",
      "[[3618   24]\n",
      " [  19 4710]]\n",
      "test accuracy: 0.5733397037744864\n",
      "test confusion matrix\n",
      "[[434 465]\n",
      " [428 766]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(pol_data_x.url,pol_data_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "train accuracy: 0.7185521443077291\n",
      "train confusion matrix\n",
      "[[2083 1559]\n",
      " [ 797 3932]]\n",
      "test accuracy: 0.590539894887721\n",
      "test confusion matrix\n",
      "[[341 558]\n",
      " [299 895]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(pol_data_x.Speaker,pol_data_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.9053876478318003\n",
      "train confusion matrix\n",
      "[[3179  463]\n",
      " [ 329 4400]]\n",
      "test accuracy: 0.5661729574773053\n",
      "test confusion matrix\n",
      "[[394 505]\n",
      " [403 791]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(pol_data_x.Statement,pol_data_y,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.9115995699438538\n",
      "train confusion matrix\n",
      "[[3214  428]\n",
      " [ 312 4417]]\n",
      "test accuracy: 0.5895843287147635\n",
      "test confusion matrix\n",
      "[[406 493]\n",
      " [366 828]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(pol_data_x.url,pol_data_y,stop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.6653924262334249\n",
      "train confusion matrix\n",
      "[[2661  981]\n",
      " [1820 2909]]\n",
      "test accuracy: 0.5437171524128046\n",
      "test confusion matrix\n",
      "[[477 422]\n",
      " [533 661]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(pol_data_x.Speaker,pol_data_y,stop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([])\n",
    "a.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_tfidf_pac(x1,x2,y,x3=a,stop=False):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    if x3.size==0:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "\n",
    "\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7)   # a TFIDF vectorizer\n",
    "        tfidf_train_x1 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x1).todense())  #fitting the training data\n",
    "        tfidf_test_x1 = pd.DataFrame(tfidf_vectorizer.transform(test_x1).todense())\n",
    "\n",
    "        tfidf_train_x2 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x2).todense())  #fitting the training data\n",
    "        tfidf_test_x2 = pd.DataFrame(tfidf_vectorizer.transform(test_x2).todense()) \n",
    "\n",
    "        train=pd.concat([tfidf_train_x1,tfidf_train_x2],axis=1)\n",
    "        test=pd.concat([tfidf_test_x1,tfidf_test_x2],axis=1)\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "        print(train_y2.shape)\n",
    "        print(test_y2.shape)\n",
    "\n",
    "        ap.passive_aggressive(train,train_y2,test,test_y2,stop)\n",
    "    \n",
    "    else:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7)   # a TFIDF vectorizer\n",
    "        tfidf_train_x1 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x1).todense())  #fitting the training data\n",
    "        tfidf_test_x1 = pd.DataFrame(tfidf_vectorizer.transform(test_x1).todense())\n",
    "\n",
    "        tfidf_train_x2 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x2).todense())  #fitting the training data\n",
    "        tfidf_test_x2 = pd.DataFrame(tfidf_vectorizer.transform(test_x2).todense()) \n",
    "        \n",
    "        tfidf_train_x3 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x3).todense())  #fitting the training data\n",
    "        tfidf_test_x3 = pd.DataFrame(tfidf_vectorizer.transform(test_x3).todense())\n",
    "        \n",
    "        train=pd.concat([tfidf_train_x1,tfidf_train_x2,tfidf_train_x3],axis=1)\n",
    "        test=pd.concat([tfidf_test_x1,tfidf_test_x2,tfidf_test_x3],axis=1)\n",
    "        \n",
    "        ap.passive_aggressive(train,train_y2,test,test_y2,stop)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 13525)\n",
      "(2093, 13525)\n",
      "(8371,)\n",
      "(2093,)\n",
      "train accuracy: 0.9990443196750687\n",
      "train confusion matrix\n",
      "[[3641    1]\n",
      " [   7 4722]]\n",
      "test accuracy: 0.5714285714285714\n",
      "test confusion matrix\n",
      "[[502 397]\n",
      " [500 694]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf_pac(x1=pol_data_x.Statement,x2=pol_data_x.Speaker,y=pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 20431)\n",
      "(2093, 20431)\n",
      "(8371,)\n",
      "(2093,)\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[3642    0]\n",
      " [   0 4729]]\n",
      "test accuracy: 0.5938843764930721\n",
      "test confusion matrix\n",
      "[[440 459]\n",
      " [391 803]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf_pac(x1=pol_data_x.Statement,x2=pol_data_x.url,y=pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 12664)\n",
      "(2093, 12664)\n",
      "(8371,)\n",
      "(2093,)\n",
      "train accuracy: 0.9962967387408912\n",
      "train confusion matrix\n",
      "[[3635    7]\n",
      " [  24 4705]]\n",
      "test accuracy: 0.5771619684663163\n",
      "test confusion matrix\n",
      "[[488 411]\n",
      " [474 720]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf_pac(x1=pol_data_x.url,x2=pol_data_x.Speaker,y=pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[3642    0]\n",
      " [   0 4729]]\n",
      "test accuracy: 0.6139512661251791\n",
      "test confusion matrix\n",
      "[[441 458]\n",
      " [350 844]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf_pac(x1=pol_data_x.Statement,x2=pol_data_x.Speaker,y=pol_data_y,x3=pol_data_x.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 13525)\n",
      "(2093, 13525)\n",
      "(8371,)\n",
      "(2093,)\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.8141201768008601\n",
      "train confusion matrix\n",
      "[[3487  155]\n",
      " [1401 3328]]\n",
      "test accuracy: 0.5441949354992833\n",
      "test confusion matrix\n",
      "[[598 301]\n",
      " [653 541]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf_pac(x1=pol_data_x.Statement,x2=pol_data_x.Speaker,y=pol_data_y,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 20431)\n",
      "(2093, 20431)\n",
      "(8371,)\n",
      "(2093,)\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.9534105841595986\n",
      "train confusion matrix\n",
      "[[3430  212]\n",
      " [ 178 4551]]\n",
      "test accuracy: 0.5972288580984233\n",
      "test confusion matrix\n",
      "[[432 467]\n",
      " [376 818]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf_pac(x1=pol_data_x.Statement,x2=pol_data_x.url,y=pol_data_y,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 12664)\n",
      "(2093, 12664)\n",
      "(8371,)\n",
      "(2093,)\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.887707561820571\n",
      "train confusion matrix\n",
      "[[3362  280]\n",
      " [ 660 4069]]\n",
      "test accuracy: 0.592451027233636\n",
      "test confusion matrix\n",
      "[[517 382]\n",
      " [471 723]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf_pac(x1=pol_data_x.url,x2=pol_data_x.Speaker,y=pol_data_y,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.9505435431848047\n",
      "train confusion matrix\n",
      "[[3482  160]\n",
      " [ 254 4475]]\n",
      "test accuracy: 0.6077400860009555\n",
      "test confusion matrix\n",
      "[[522 377]\n",
      " [444 750]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf_pac(x1=pol_data_x.Statement,x2=pol_data_x.Speaker,y=pol_data_y,x3=pol_data_x.url,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pac hash now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "train accuracy: 0.984111814598017\n",
      "train confusion matrix\n",
      "[[3560   82]\n",
      " [  51 4678]]\n",
      "test accuracy: 0.5470616340181558\n",
      "test confusion matrix\n",
      "[[430 469]\n",
      " [479 715]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(pol_data_x.Statement,pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "train accuracy: 0.9886512961414406\n",
      "train confusion matrix\n",
      "[[3568   74]\n",
      " [  21 4708]]\n",
      "test accuracy: 0.5733397037744864\n",
      "test confusion matrix\n",
      "[[397 502]\n",
      " [391 803]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(pol_data_x.url,pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "train accuracy: 0.7103094015051965\n",
      "train confusion matrix\n",
      "[[2413 1229]\n",
      " [1196 3533]]\n",
      "test accuracy: 0.5747730530339226\n",
      "test confusion matrix\n",
      "[[415 484]\n",
      " [406 788]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(pol_data_x.Speaker,pol_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.8535419902042767\n",
      "train confusion matrix\n",
      "[[2772  870]\n",
      " [ 356 4373]]\n",
      "test accuracy: 0.5628284758719542\n",
      "test confusion matrix\n",
      "[[349 550]\n",
      " [365 829]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(pol_data_x.Statement,pol_data_y,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.882809700155298\n",
      "train confusion matrix\n",
      "[[3057  585]\n",
      " [ 396 4333]]\n",
      "test accuracy: 0.5910176779741997\n",
      "test confusion matrix\n",
      "[[415 484]\n",
      " [372 822]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(pol_data_x.url,pol_data_y,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 8371 / 2093\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.6997969179309521\n",
      "train confusion matrix\n",
      "[[1875 1767]\n",
      " [ 746 3983]]\n",
      "test accuracy: 0.5943621595795509\n",
      "test confusion matrix\n",
      "[[311 588]\n",
      " [261 933]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(pol_data_x.Speaker,pol_data_y,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hashv_pac(x1,x2,y,x3=a,stop=False):\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    # Not sure about this, collisions may occur as I forced absolute cause naivie bayes is a bitch and n_features over \n",
    "                                                                                        #2**15 causes memory error\n",
    "    if x3.size==0:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "\n",
    "\n",
    "\n",
    "        hash_vectorizer = HashingVectorizer(stop_words='english',n_features = 2**15)   # a HASH vectorizer\n",
    "        hash_train_x1 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x1).todense()))  #fitting the training data\n",
    "        hash_test_x1 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x1).todense()))\n",
    "\n",
    "        hash_train_x2 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x2).todense()))  #fitting the training data\n",
    "        hash_test_x2 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x2).todense())) \n",
    "\n",
    "        train=pd.concat([hash_train_x1,hash_train_x2],axis=1)\n",
    "        test=pd.concat([hash_test_x1,hash_test_x2],axis=1)\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "        print(train_y2.shape)\n",
    "        print(test_y2.shape)\n",
    "\n",
    "        ap.passive_aggressive(train,train_y2,test,test_y2,stop)    \n",
    "    \n",
    "    else:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        \n",
    "        hash_vectorizer = HashingVectorizer(stop_words='english',n_features = 2**15)   # a HASH vectorizer\n",
    "        hash_train_x1 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x1).todense()) ) #fitting the training data\n",
    "        hash_test_x1 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x1).todense()))\n",
    "\n",
    "        hash_train_x2 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x2).todense()) ) #fitting the training data\n",
    "        hash_test_x2 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x2).todense()) )\n",
    "        \n",
    "        hash_train_x3 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x3).todense()))  #fitting the training data\n",
    "        hash_test_x3 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x3).todense()))\n",
    "        \n",
    "        train=pd.concat([hash_train_x1,hash_train_x2,hash_train_x3],axis=1)\n",
    "        test=pd.concat([hash_test_x1,hash_test_x2,hash_test_x3],axis=1)\n",
    "        \n",
    "        ap.passive_aggressive(train,train_y2,test,test_y2,stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 65536)\n",
      "(2093, 65536)\n",
      "(8371,)\n",
      "(2093,)\n",
      "train accuracy: 0.9960578186596584\n",
      "train confusion matrix\n",
      "[[3616   26]\n",
      " [   7 4722]]\n",
      "test accuracy: 0.5805064500716675\n",
      "test confusion matrix\n",
      "[[424 475]\n",
      " [403 791]]\n"
     ]
    }
   ],
   "source": [
    "multi_hashv_pac(x1=pol_data_x.Statement,x2=pol_data_x.Speaker,y=pol_data_y,x3=a,stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 65536)\n",
      "(2093, 65536)\n",
      "(8371,)\n",
      "(2093,)\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.8430295066300323\n",
      "train confusion matrix\n",
      "[[2995  647]\n",
      " [ 667 4062]]\n",
      "test accuracy: 0.5891065456282848\n",
      "test confusion matrix\n",
      "[[437 462]\n",
      " [398 796]]\n"
     ]
    }
   ],
   "source": [
    "multi_hashv_pac(x1=pol_data_x.Statement,x2=pol_data_x.Speaker,y=pol_data_y,x3=a,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 65536)\n",
      "(2093, 65536)\n",
      "(8371,)\n",
      "(2093,)\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[3642    0]\n",
      " [   0 4729]]\n",
      "test accuracy: 0.5871954132823698\n",
      "test confusion matrix\n",
      "[[444 455]\n",
      " [409 785]]\n"
     ]
    }
   ],
   "source": [
    "multi_hashv_pac(x1=pol_data_x.Statement,x2=pol_data_x.url,y=pol_data_y,x3=a,stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 65536)\n",
      "(2093, 65536)\n",
      "(8371,)\n",
      "(2093,)\n",
      "train accuracy: 0.9934296977660972\n",
      "train confusion matrix\n",
      "[[3605   37]\n",
      " [  18 4711]]\n",
      "test accuracy: 0.5876731963688485\n",
      "test confusion matrix\n",
      "[[413 486]\n",
      " [377 817]]\n"
     ]
    }
   ],
   "source": [
    "multi_hashv_pac(x1=pol_data_x.url,x2=pol_data_x.Speaker,y=pol_data_y,x3=a,stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 65536)\n",
      "(2093, 65536)\n",
      "(8371,)\n",
      "(2093,)\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.9157806713654283\n",
      "train confusion matrix\n",
      "[[3287  355]\n",
      " [ 350 4379]]\n",
      "test accuracy: 0.5852842809364549\n",
      "test confusion matrix\n",
      "[[436 463]\n",
      " [405 789]]\n"
     ]
    }
   ],
   "source": [
    "multi_hashv_pac(x1=pol_data_x.Statement,x2=pol_data_x.url,y=pol_data_y,x3=a,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "(8371, 65536)\n",
      "(2093, 65536)\n",
      "(8371,)\n",
      "(2093,)\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.8280970015529805\n",
      "train confusion matrix\n",
      "[[3152  490]\n",
      " [ 949 3780]]\n",
      "test accuracy: 0.5814620162446249\n",
      "test confusion matrix\n",
      "[[492 407]\n",
      " [469 725]]\n"
     ]
    }
   ],
   "source": [
    "multi_hashv_pac(x1=pol_data_x.url,x2=pol_data_x.Speaker,y=pol_data_y,x3=a,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.8659658344283837\n",
      "train confusion matrix\n",
      "[[3489  153]\n",
      " [ 969 3760]]\n",
      "test accuracy: 0.5661729574773053\n",
      "test confusion matrix\n",
      "[[601 298]\n",
      " [610 584]]\n"
     ]
    }
   ],
   "source": [
    "multi_hashv_pac(x1=pol_data_x.url,x2=pol_data_x.Speaker,y=pol_data_y,stop=True,x3=pol_data_x.Statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "Splitting\n",
      "Final size of train/test : 8371 / 2093\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[3642    0]\n",
      " [   0 4729]]\n",
      "test accuracy: 0.6005733397037745\n",
      "test confusion matrix\n",
      "[[441 458]\n",
      " [378 816]]\n"
     ]
    }
   ],
   "source": [
    "multi_hashv_pac(x1=pol_data_x.url,x2=pol_data_x.Speaker,y=pol_data_y,stop=False,x3=pol_data_x.Statement)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
