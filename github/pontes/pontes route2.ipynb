{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from auto_naive.ipynb\n",
      "importing Jupyter notebook from auto_pac.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "import auto_naive as an\n",
    "import auto_pac as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (1,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dataset=pd.read_csv('datasets/pontes set/resized_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426550, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['id', 'Unnamed: 0'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['domain', 'type', 'url', 'content', 'scraped_at', 'inserted_at',\n",
       "       'updated_at', 'title', 'authors', 'keywords', 'meta_keywords',\n",
       "       'meta_description', 'tags', 'summary', 'source'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain                0.001172\n",
       "type                  4.723948\n",
       "url                   0.001172\n",
       "content               0.001172\n",
       "scraped_at            0.001407\n",
       "inserted_at           0.001407\n",
       "updated_at            0.001407\n",
       "title                 0.853827\n",
       "authors              44.418708\n",
       "keywords            100.000000\n",
       "meta_keywords         4.017583\n",
       "meta_description     52.563357\n",
       "tags                 76.969640\n",
       "summary             100.000000\n",
       "source               77.875982\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()/(len(dataset))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['url', 'scraped_at', 'inserted_at', 'updated_at','keywords', 'meta_keywords',\n",
    "       'meta_description', 'tags', 'summary'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain      0.001172\n",
       "type        4.723948\n",
       "content     0.001172\n",
       "title       0.853827\n",
       "authors    44.418708\n",
       "source     77.875982\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()/(len(dataset))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['source'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain      0.001172\n",
       "type        4.723948\n",
       "content     0.001172\n",
       "title       0.853827\n",
       "authors    44.418708\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()/(len(dataset))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426550, 5)\n",
      "2132750\n",
      "426550\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(dataset.size)\n",
    "print(dataset.domain.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224847, 5)\n",
      "1124235\n",
      "224847\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "print(dataset.size)\n",
    "print(dataset.domain.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain     0.0\n",
       "type       0.0\n",
       "content    0.0\n",
       "title      0.0\n",
       "authors    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()/(len(dataset))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['satire', 'unreliable', 'hate', 'clickbait', 'conspiracy', 'fake',\n",
       "       'political', 'bias', 'rumor', 'reliable', 'junksci', 'unknown'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_1=r2[r2.type == 'fake']\n",
    "d_2=r2[r2.type == 'clickbait']\n",
    "d_3=r2[r2.type == 'reliable']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040 31125 298925\n"
     ]
    }
   ],
   "source": [
    "print(d_1.size,d_2.size,d_3.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_clean=pd.concat([d_1,d_2,d_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93018, 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_clean.to_csv('datasets/pontes set/route2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RErun from here for route 2\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "route2=pd.read_csv('datasets/pontes set/route2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93018, 5)\n",
      "Index(['domain', 'type', 'content', 'title', 'authors'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(route2.shape)\n",
    "print(route2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "route2.replace({'clickbait':'fake'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fake', 'reliable'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route2.type.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "route2.rename(columns={\"type\": \"label\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "route2_y=route2.label\n",
    "route2_x=route2.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93018, 4)\n",
      "(93018,)\n",
      "Index(['domain', 'label', 'content', 'title', 'authors'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(route2_x.shape)\n",
    "print(route2_y.shape)\n",
    "print(route2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBuilding models for Route 2 Naive Bayes\\n\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Building models for Route 2 Naive Bayes\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "train_x : (74414,)\n",
      "train_y : (74414,)\n",
      "test_x : (18604,)\n",
      "test_y : (18604,)\n",
      "Vectorizing\n",
      "test_x : (18604,)\n",
      "train_x : (74414,)\n",
      "tfidf_train_x :  (74414, 116)\n",
      "tfidf_test_x :  (18604, 116)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 1.0\n",
      "grid alpha 0.001\n",
      "Alpha: 0.001 test_Score: 1.00000 train_score: 1.00000\n",
      "training confusion matrix\n",
      "[[26483     0]\n",
      " [    0 47931]]\n",
      "testing confusion matrix\n",
      "[[ 6750     0]\n",
      " [    0 11854]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_tfidf_model(route2_x.domain,route2_y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "train_x : (74414,)\n",
      "train_y : (74414,)\n",
      "test_x : (18604,)\n",
      "test_y : (18604,)\n",
      "Vectorizing\n",
      "test_x : (18604,)\n",
      "train_x : (74414,)\n",
      "tfidf_train_x :  (74414, 45409)\n",
      "tfidf_test_x :  (18604, 45409)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.7869083774558551\n",
      "grid alpha 0.171\n",
      "Alpha: 0.171 test_Score: 0.79193 train_score: 0.89138\n",
      "training confusion matrix\n",
      "[[20028  6455]\n",
      " [ 1628 46303]]\n",
      "testing confusion matrix\n",
      "[[ 3870  2880]\n",
      " [  991 10863]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_tfidf_model(route2_x.title,route2_y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "train_x : (74414,)\n",
      "train_y : (74414,)\n",
      "test_x : (18604,)\n",
      "test_y : (18604,)\n",
      "Vectorizing\n",
      "test_x : (18604,)\n",
      "train_x : (74414,)\n",
      "tfidf_train_x :  (74414, 13813)\n",
      "tfidf_test_x :  (18604, 13813)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.963904641599699\n",
      "grid alpha 0.628\n",
      "Alpha: 0.628 test_Score: 0.96764 train_score: 0.97703\n",
      "training confusion matrix\n",
      "[[25216  1267]\n",
      " [  442 47489]]\n",
      "testing confusion matrix\n",
      "[[ 6308   442]\n",
      " [  160 11694]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_tfidf_model(route2_x.authors,route2_y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "train_x : (74414,)\n",
      "train_y : (74414,)\n",
      "test_x : (18604,)\n",
      "test_y : (18604,)\n",
      "Vectorizing\n",
      "test_x : (18604,)\n",
      "train_x : (74414,)\n",
      "tfidf_train_x :  (74414, 305793)\n",
      "tfidf_test_x :  (18604, 305793)\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.9240196737173113\n",
      "grid alpha 0.012\n",
      "Alpha: 0.012 test_Score: 0.92131 train_score: 0.96924\n",
      "training confusion matrix\n",
      "[[25004  1479]\n",
      " [  810 47121]]\n",
      "testing confusion matrix\n",
      "[[ 5715  1035]\n",
      " [  429 11425]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_tfidf_model(route2_x.content,route2_y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([])\n",
    "a.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_tfidf(x1,x2,y,x3=a,itera=100):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    if x3.size==0:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "\n",
    "\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7,min_df=0.02)   # a TFIDF vectorizer\n",
    "        tfidf_train_x1 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x1).todense())  #fitting the training data\n",
    "        tfidf_test_x1 = pd.DataFrame(tfidf_vectorizer.transform(test_x1).todense())\n",
    "\n",
    "        tfidf_train_x2 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x2).todense())  #fitting the training data\n",
    "        tfidf_test_x2 = pd.DataFrame(tfidf_vectorizer.transform(test_x2).todense()) \n",
    "\n",
    "        train=pd.concat([tfidf_train_x1,tfidf_train_x2],axis=1)\n",
    "        test=pd.concat([tfidf_test_x1,tfidf_test_x2],axis=1)\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "        print(train_y2.shape)\n",
    "        print(test_y2.shape)\n",
    "\n",
    "        an.navie_bayes(train,train_y2,test,test_y2,itera)\n",
    "    \n",
    "    else:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7,min_df=0.02)   # a TFIDF vectorizer\n",
    "        tfidf_train_x1 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x1).todense())  #fitting the training data\n",
    "        tfidf_test_x1 = pd.DataFrame(tfidf_vectorizer.transform(test_x1).todense())\n",
    "\n",
    "        tfidf_train_x2 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x2).todense())  #fitting the training data\n",
    "        tfidf_test_x2 = pd.DataFrame(tfidf_vectorizer.transform(test_x2).todense()) \n",
    "        \n",
    "        tfidf_train_x3 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x3).todense())  #fitting the training data\n",
    "        tfidf_test_x3 = pd.DataFrame(tfidf_vectorizer.transform(test_x3).todense())\n",
    "        \n",
    "        train=pd.concat([tfidf_train_x1,tfidf_train_x2,tfidf_train_x3],axis=1)\n",
    "        test=pd.concat([tfidf_test_x1,tfidf_test_x2,tfidf_test_x3],axis=1)\n",
    "        \n",
    "        an.navie_bayes(train,train_y2,test,test_y2,itera)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.9690112075684683\n",
      "grid alpha 0.9\n",
      "Alpha: 0.900 test_Score: 0.97054 train_score: 0.96944\n",
      "training confusion matrix\n",
      "[[24741  1742]\n",
      " [  532 47399]]\n",
      "testing confusion matrix\n",
      "[[ 6325   425]\n",
      " [  123 11731]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf(x1=route2_x.domain,x2=route2_x.content,x3=route2_x.authors,y=route2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_tfidf4(x1,x2,y,x3,x4,itera=100):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        train_x4,train_y4,test_x4,test_y4=an.split_data(x3,y)\n",
    "\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7,min_df=0.02)   # a TFIDF vectorizer\n",
    "        \n",
    "        tfidf_train_x1 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x1).todense())  #fitting the training data\n",
    "        tfidf_test_x1 = pd.DataFrame(tfidf_vectorizer.transform(test_x1).todense())\n",
    "\n",
    "        tfidf_train_x2 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x2).todense())  #fitting the training data\n",
    "        tfidf_test_x2 = pd.DataFrame(tfidf_vectorizer.transform(test_x2).todense()) \n",
    "        \n",
    "        tfidf_train_x3 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x3).todense())  #fitting the training data\n",
    "        tfidf_test_x3 = pd.DataFrame(tfidf_vectorizer.transform(test_x3).todense())\n",
    "        \n",
    "        tfidf_train_x4 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x4).todense())  #fitting the training data\n",
    "        tfidf_test_x4 = pd.DataFrame(tfidf_vectorizer.transform(test_x4).todense())\n",
    "        \n",
    "        train=pd.concat([tfidf_train_x1,tfidf_train_x2,tfidf_train_x3,tfidf_train_x4],axis=1)\n",
    "        test=pd.concat([tfidf_test_x1,tfidf_test_x2,tfidf_test_x3,tfidf_test_x4],axis=1)\n",
    "        \n",
    "        an.navie_bayes(train,train_y2,test,test_y2,itera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.9681914693471658\n",
      "grid alpha 0.13\n",
      "Alpha: 0.130 test_Score: 0.96968 train_score: 0.96859\n",
      "training confusion matrix\n",
      "[[24715  1768]\n",
      " [  569 47362]]\n",
      "testing confusion matrix\n",
      "[[ 6315   435]\n",
      " [  129 11725]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf4(x1=route2_x.domain,x2=route2_x.content,x3=route2_x.authors,y=route2_y,x4=route2_x.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb hash model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Vectorizing\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.9999328083425162\n",
      "grid alpha 0.001\n",
      "Alpha: 0.001 test_Score: 1.00000 train_score: 1.00000\n",
      "training confusion matrix\n",
      "[[26483     0]\n",
      " [    0 47931]]\n",
      "testing confusion matrix\n",
      "[[ 6750     0]\n",
      " [    0 11854]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_hash_model(route2_x.domain,route2_y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Vectorizing\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.7776090520600962\n",
      "grid alpha 0.012\n",
      "Alpha: 0.012 test_Score: 0.78693 train_score: 0.89194\n",
      "training confusion matrix\n",
      "[[19824  6659]\n",
      " [ 1382 46549]]\n",
      "testing confusion matrix\n",
      "[[ 3849  2901]\n",
      " [ 1063 10791]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_hash_model(route2_x.title,route2_y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Vectorizing\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.9680436477007015\n",
      "grid alpha 0.027000000000000003\n",
      "Alpha: 0.027 test_Score: 0.96850 train_score: 0.97971\n",
      "training confusion matrix\n",
      "[[25288  1195]\n",
      " [  315 47616]]\n",
      "testing confusion matrix\n",
      "[[ 6312   438]\n",
      " [  148 11706]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_hash_model(route2_x.authors,route2_y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Vectorizing\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.9333189991130701\n",
      "grid alpha 0.002\n",
      "Alpha: 0.002 test_Score: 0.93254 train_score: 0.96290\n",
      "training confusion matrix\n",
      "[[25006  1477]\n",
      " [ 1284 46647]]\n",
      "testing confusion matrix\n",
      "[[ 6010   740]\n",
      " [  515 11339]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "an.navie_bayes_hash_model(route2_x.content,route2_y,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_hash4(x1,x2,y,x3,x4,itera=100):\n",
    "        from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        train_x4,train_y4,test_x4,test_y4=an.split_data(x3,y)\n",
    "\n",
    "        \n",
    "        hash_vectorizer = HashingVectorizer(stop_words='english',n_features=2**8)   # a HASH vectorizer\n",
    "        \n",
    "        hash_train_x1 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x1).todense()))  #fitting the training data\n",
    "        hash_test_x1 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x1).todense()))\n",
    "\n",
    "        hash_train_x2 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x2).todense()) ) #fitting the training data\n",
    "        hash_test_x2 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x2).todense()) )\n",
    "        \n",
    "        hash_train_x3 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x3).todense()))  #fitting the training data\n",
    "        hash_test_x3 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x3).todense()))\n",
    "        \n",
    "        hash_train_x4 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x4).todense()) ) #fitting the training data\n",
    "        hash_test_x4 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x4).todense()))\n",
    "        \n",
    "        train=pd.concat([hash_train_x1,hash_train_x2,hash_train_x3,hash_train_x4],axis=1)\n",
    "        test=pd.concat([hash_test_x1,hash_test_x2,hash_test_x3,hash_test_x4],axis=1)\n",
    "        \n",
    "        an.navie_bayes(train,train_y2,test,test_y2,itera)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.9973123337006478\n",
      "grid alpha 0.01\n",
      "Alpha: 0.010 test_Score: 0.99699 train_score: 0.99734\n",
      "training confusion matrix\n",
      "[[26285   198]\n",
      " [    0 47931]]\n",
      "testing confusion matrix\n",
      "[[ 6694    56]\n",
      " [    0 11854]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mult_hash4(x1=route2_x.domain,x2=route2_x.content,x3=route2_x.authors,y=route2_y,x4=route2_x.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hashv(x1,x2,y,x3=a):\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "  \n",
    "    if x3.size==0:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "\n",
    "\n",
    "\n",
    "        hash_vectorizer = HashingVectorizer(stop_words='english',n_features = 2**8)   # a HASH vectorizer\n",
    "        hash_train_x1 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x1).todense()))  #fitting the training data\n",
    "        hash_test_x1 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x1).todense()))\n",
    "\n",
    "        hash_train_x2 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x2).todense()))  #fitting the training data\n",
    "        hash_test_x2 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x2).todense())) \n",
    "\n",
    "        train=pd.concat([hash_train_x1,hash_train_x2],axis=1)\n",
    "        test=pd.concat([hash_test_x1,hash_test_x2],axis=1)\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "        print(train_y2.shape)\n",
    "        print(test_y2.shape)\n",
    "\n",
    "        an.navie_bayes(train,train_y2,test,test_y2,100)\n",
    "    \n",
    "    else:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        \n",
    "        hash_vectorizer = HashingVectorizer(stop_words='english',n_features = 2**8)   # a HASH vectorizer\n",
    "        hash_train_x1 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x1).todense()) ) #fitting the training data\n",
    "        hash_test_x1 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x1).todense()))\n",
    "\n",
    "        hash_train_x2 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x2).todense()) ) #fitting the training data\n",
    "        hash_test_x2 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x2).todense()) )\n",
    "        \n",
    "        hash_train_x3 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x3).todense()))  #fitting the training data\n",
    "        hash_test_x3 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x3).todense()))\n",
    "        \n",
    "        train=pd.concat([hash_train_x1,hash_train_x2,hash_train_x3],axis=1)\n",
    "        test=pd.concat([hash_test_x1,hash_test_x2,hash_test_x3],axis=1)\n",
    "        \n",
    "        an.navie_bayes(train,train_y2,test,test_y2,100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "model building\n",
      "begin grid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid score 0.9962372671809068\n",
      "grid alpha 0.01\n",
      "Alpha: 0.010 test_Score: 0.99581 train_score: 0.99630\n",
      "training confusion matrix\n",
      "[[26208   275]\n",
      " [    0 47931]]\n",
      "testing confusion matrix\n",
      "[[ 6672    78]\n",
      " [    0 11854]]\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "multi_hashv(x1=route2_x.domain,x2=route2_x.content,x3=route2_x.authors,y=route2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "unto pac tfidf\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[26483     0]\n",
      " [    0 47931]]\n",
      "test accuracy: 1.0\n",
      "test confusion matrix\n",
      "[[ 6750     0]\n",
      " [    0 11854]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(route2_x.domain,route2_y,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 0.9999865616685032\n",
      "train confusion matrix\n",
      "[[26483     0]\n",
      " [    1 47930]]\n",
      "test accuracy: 0.9609761341646957\n",
      "test confusion matrix\n",
      "[[ 6365   385]\n",
      " [  341 11513]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(route2_x.content,route2_y,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 0.9951487623296692\n",
      "train confusion matrix\n",
      "[[26233   250]\n",
      " [  111 47820]]\n",
      "test accuracy: 0.9777467211352397\n",
      "test confusion matrix\n",
      "[[ 6540   210]\n",
      " [  204 11650]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(route2_x.authors,route2_y,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 0.9573467358292794\n",
      "train confusion matrix\n",
      "[[24489  1994]\n",
      " [ 1180 46751]]\n",
      "test accuracy: 0.7530101053536874\n",
      "test confusion matrix\n",
      "[[4188 2562]\n",
      " [2033 9821]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(route2_x.title,route2_y,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[26483     0]\n",
      " [    0 47931]]\n",
      "test accuracy: 1.0\n",
      "test confusion matrix\n",
      "[[ 6750     0]\n",
      " [    0 11854]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(route2_x.domain,route2_y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.9961431988604295\n",
      "train confusion matrix\n",
      "[[26358   125]\n",
      " [  162 47769]]\n",
      "test accuracy: 0.9604923672328531\n",
      "test confusion matrix\n",
      "[[ 6385   365]\n",
      " [  370 11484]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(route2_x.content,route2_y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.992568602682291\n",
      "train confusion matrix\n",
      "[[26138   345]\n",
      " [  208 47723]]\n",
      "test accuracy: 0.9758654052891851\n",
      "test confusion matrix\n",
      "[[ 6513   237]\n",
      " [  212 11642]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(route2_x.authors,route2_y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.9186443411186067\n",
      "train confusion matrix\n",
      "[[22696  3787]\n",
      " [ 2267 45664]]\n",
      "test accuracy: 0.7723070307460761\n",
      "test confusion matrix\n",
      "[[ 4230  2520]\n",
      " [ 1716 10138]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_tfidf_model(route2_x.title,route2_y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_tfidf_pac(x1,x2,y,x3=a,stop=False):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    if x3.size==0:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "\n",
    "\n",
    "\n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7,min_df=0.02)   # a TFIDF vectorizer\n",
    "        tfidf_train_x1 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x1).todense())  #fitting the training data\n",
    "        tfidf_test_x1 = pd.DataFrame(tfidf_vectorizer.transform(test_x1).todense())\n",
    "\n",
    "        tfidf_train_x2 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x2).todense())  #fitting the training data\n",
    "        tfidf_test_x2 = pd.DataFrame(tfidf_vectorizer.transform(test_x2).todense()) \n",
    "\n",
    "        train=pd.concat([tfidf_train_x1,tfidf_train_x2],axis=1)\n",
    "        test=pd.concat([tfidf_test_x1,tfidf_test_x2],axis=1)\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "        print(train_y2.shape)\n",
    "        print(test_y2.shape)\n",
    "\n",
    "        ap.passive_aggressive(train,train_y2,test,test_y2,stop)\n",
    "    \n",
    "    else:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7,min_df=0.02)   # a TFIDF vectorizer\n",
    "        tfidf_train_x1 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x1).todense())  #fitting the training data\n",
    "        tfidf_test_x1 = pd.DataFrame(tfidf_vectorizer.transform(test_x1).todense())\n",
    "\n",
    "        tfidf_train_x2 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x2).todense())  #fitting the training data\n",
    "        tfidf_test_x2 = pd.DataFrame(tfidf_vectorizer.transform(test_x2).todense()) \n",
    "        \n",
    "        tfidf_train_x3 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x3).todense())  #fitting the training data\n",
    "        tfidf_test_x3 = pd.DataFrame(tfidf_vectorizer.transform(test_x3).todense())\n",
    "        \n",
    "        train=pd.concat([tfidf_train_x1,tfidf_train_x2,tfidf_train_x3],axis=1)\n",
    "        test=pd.concat([tfidf_test_x1,tfidf_test_x2,tfidf_test_x3],axis=1)\n",
    "        \n",
    "        ap.passive_aggressive(train,train_y2,test,test_y2,stop)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 0.9940333808154379\n",
      "train confusion matrix\n",
      "[[26047   436]\n",
      " [    8 47923]]\n",
      "test accuracy: 0.9855407439260374\n",
      "test confusion matrix\n",
      "[[ 6545   205]\n",
      " [   64 11790]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf_pac(x1=route2_x.domain,x2=route2_x.content,x3=route2_x.authors,y=route2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.9959013088934878\n",
      "train confusion matrix\n",
      "[[26240   243]\n",
      " [   62 47869]]\n",
      "test accuracy: 0.9878520748226188\n",
      "test confusion matrix\n",
      "[[ 6607   143]\n",
      " [   83 11771]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf_pac(x1=route2_x.domain,x2=route2_x.content,x3=route2_x.authors,y=route2_y,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_tfidf4_pac(x1,x2,y,x3,x4,stop=False):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        train_x4,train_y4,test_x4,test_y4=an.split_data(x3,y)\n",
    "\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words='english',max_df=0.7,min_df=0.02)   # a TFIDF vectorizer\n",
    "        \n",
    "        tfidf_train_x1 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x1).todense())  #fitting the training data\n",
    "        tfidf_test_x1 = pd.DataFrame(tfidf_vectorizer.transform(test_x1).todense())\n",
    "\n",
    "        tfidf_train_x2 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x2).todense())  #fitting the training data\n",
    "        tfidf_test_x2 = pd.DataFrame(tfidf_vectorizer.transform(test_x2).todense()) \n",
    "        \n",
    "        tfidf_train_x3 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x3).todense())  #fitting the training data\n",
    "        tfidf_test_x3 = pd.DataFrame(tfidf_vectorizer.transform(test_x3).todense())\n",
    "        \n",
    "        tfidf_train_x4 = pd.DataFrame(tfidf_vectorizer.fit_transform(train_x4).todense())  #fitting the training data\n",
    "        tfidf_test_x4 = pd.DataFrame(tfidf_vectorizer.transform(test_x4).todense())\n",
    "        \n",
    "        train=pd.concat([tfidf_train_x1,tfidf_train_x2,tfidf_train_x3,tfidf_train_x4],axis=1)\n",
    "        test=pd.concat([tfidf_test_x1,tfidf_test_x2,tfidf_test_x3,tfidf_test_x4],axis=1)\n",
    "        \n",
    "        ap.passive_aggressive(train,train_y2,test,test_y2,stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 0.9988039884967882\n",
      "train confusion matrix\n",
      "[[26472    11]\n",
      " [   78 47853]]\n",
      "test accuracy: 0.9893571274994625\n",
      "test confusion matrix\n",
      "[[ 6680    70]\n",
      " [  128 11726]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf4_pac(x1=route2_x.domain,x2=route2_x.content,x3=route2_x.authors,y=route2_y,x4=route2_x.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 0.9988443034912785\n",
      "train confusion matrix\n",
      "[[26455    28]\n",
      " [   58 47873]]\n",
      "test accuracy: 0.9890346162115674\n",
      "test confusion matrix\n",
      "[[ 6664    86]\n",
      " [  118 11736]]\n"
     ]
    }
   ],
   "source": [
    "mult_tfidf4_pac(x1=route2_x.domain,x2=route2_x.content,x3=route2_x.authors,y=route2_y,x4=route2_x.title,stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pac hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[26483     0]\n",
      " [    0 47931]]\n",
      "test accuracy: 1.0\n",
      "test confusion matrix\n",
      "[[ 6750     0]\n",
      " [    0 11854]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(route2_x.domain,route2_y,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 0.999946246674013\n",
      "train confusion matrix\n",
      "[[26483     0]\n",
      " [    4 47927]]\n",
      "test accuracy: 0.957052246828639\n",
      "test confusion matrix\n",
      "[[ 6352   398]\n",
      " [  401 11453]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(route2_x.content,route2_y,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 0.9944365307603408\n",
      "train confusion matrix\n",
      "[[26187   296]\n",
      " [  118 47813]]\n",
      "test accuracy: 0.9784992474736616\n",
      "test confusion matrix\n",
      "[[ 6527   223]\n",
      " [  177 11677]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(route2_x.authors,route2_y,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 0.9501975434730023\n",
      "train confusion matrix\n",
      "[[24303  2180]\n",
      " [ 1526 46405]]\n",
      "test accuracy: 0.759460331111589\n",
      "test confusion matrix\n",
      "[[4247 2503]\n",
      " [1972 9882]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(route2_x.title,route2_y,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[26483     0]\n",
      " [    0 47931]]\n",
      "test accuracy: 1.0\n",
      "test confusion matrix\n",
      "[[ 6750     0]\n",
      " [    0 11854]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(route2_x.domain,route2_y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.993576477544548\n",
      "train confusion matrix\n",
      "[[26190   293]\n",
      " [  185 47746]]\n",
      "test accuracy: 0.9578047731670608\n",
      "test confusion matrix\n",
      "[[ 6310   440]\n",
      " [  345 11509]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(route2_x.content,route2_y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.9917623027924853\n",
      "train confusion matrix\n",
      "[[26035   448]\n",
      " [  165 47766]]\n",
      "test accuracy: 0.9783917437110299\n",
      "test confusion matrix\n",
      "[[ 6495   255]\n",
      " [  147 11707]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(route2_x.authors,route2_y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 0.9026124116429705\n",
      "train confusion matrix\n",
      "[[22437  4046]\n",
      " [ 3201 44730]]\n",
      "test accuracy: 0.7699419479681789\n",
      "test confusion matrix\n",
      "[[ 4310  2440]\n",
      " [ 1840 10014]]\n"
     ]
    }
   ],
   "source": [
    "ap.pac_hash_model(route2_x.title,route2_y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_hashv_pac(x1,x2,y,x3=a,stop=False):\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    # Not sure about this, collisions may occur as I forced absolute cause naivie bayes is a bitch and n_features over \n",
    "                                                                                        #2**15 causes memory error\n",
    "    if x3.size==0:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "\n",
    "\n",
    "\n",
    "        hash_vectorizer = HashingVectorizer(stop_words='english',n_features = 2**8)   # a HASH vectorizer\n",
    "        hash_train_x1 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x1).todense()))  #fitting the training data\n",
    "        hash_test_x1 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x1).todense()))\n",
    "\n",
    "        hash_train_x2 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x2).todense()))  #fitting the training data\n",
    "        hash_test_x2 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x2).todense())) \n",
    "\n",
    "        train=pd.concat([hash_train_x1,hash_train_x2],axis=1)\n",
    "        test=pd.concat([hash_test_x1,hash_test_x2],axis=1)\n",
    "        print(train.shape)\n",
    "        print(test.shape)\n",
    "        print(train_y2.shape)\n",
    "        print(test_y2.shape)\n",
    "\n",
    "        ap.passive_aggressive(train,train_y2,test,test_y2,stop)    \n",
    "    \n",
    "    else:\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        \n",
    "        hash_vectorizer = HashingVectorizer(stop_words='english',n_features = 2**8)   # a HASH vectorizer\n",
    "        hash_train_x1 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x1).todense()) ) #fitting the training data\n",
    "        hash_test_x1 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x1).todense()))\n",
    "\n",
    "        hash_train_x2 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x2).todense()) ) #fitting the training data\n",
    "        hash_test_x2 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x2).todense()) )\n",
    "        \n",
    "        hash_train_x3 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x3).todense()))  #fitting the training data\n",
    "        hash_test_x3 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x3).todense()))\n",
    "        \n",
    "        train=pd.concat([hash_train_x1,hash_train_x2,hash_train_x3],axis=1)\n",
    "        test=pd.concat([hash_test_x1,hash_test_x2,hash_test_x3],axis=1)\n",
    "        \n",
    "        ap.passive_aggressive(train,train_y2,test,test_y2,stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[26483     0]\n",
      " [    0 47931]]\n",
      "test accuracy: 1.0\n",
      "test confusion matrix\n",
      "[[ 6750     0]\n",
      " [    0 11854]]\n"
     ]
    }
   ],
   "source": [
    "multi_hashv_pac(x1=route2_x.content,x2=route2_x.authors,y=route2_y,x3=route2_x.domain,stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[26483     0]\n",
      " [    0 47931]]\n",
      "test accuracy: 1.0\n",
      "test confusion matrix\n",
      "[[ 6750     0]\n",
      " [    0 11854]]\n"
     ]
    }
   ],
   "source": [
    "multi_hashv_pac(x1=route2_x.content,x2=route2_x.authors,y=route2_y,x3=route2_x.domain,stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_hash4_pac(x1,x2,y,x3,x4,stop=False):\n",
    "        from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "        train_x1,train_y1,test_x1,test_y1=an.split_data(x1,y)\n",
    "        train_x2,train_y2,test_x2,test_y2=an.split_data(x2,y)\n",
    "        train_x3,train_y3,test_x3,test_y3=an.split_data(x3,y)\n",
    "        train_x4,train_y4,test_x4,test_y4=an.split_data(x3,y)\n",
    "\n",
    "        \n",
    "        hash_vectorizer = HashingVectorizer(stop_words='english',n_features=2**8)   # a HASH vectorizer\n",
    "        \n",
    "        hash_train_x1 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x1).todense()))  #fitting the training data\n",
    "        hash_test_x1 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x1).todense()))\n",
    "\n",
    "        hash_train_x2 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x2).todense()) ) #fitting the training data\n",
    "        hash_test_x2 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x2).todense()) )\n",
    "        \n",
    "        hash_train_x3 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x3).todense()))  #fitting the training data\n",
    "        hash_test_x3 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x3).todense()))\n",
    "        \n",
    "        hash_train_x4 = pd.DataFrame(np.absolute(hash_vectorizer.fit_transform(train_x4).todense()) ) #fitting the training data\n",
    "        hash_test_x4 = pd.DataFrame(np.absolute(hash_vectorizer.transform(test_x4).todense()))\n",
    "        \n",
    "        train=pd.concat([hash_train_x1,hash_train_x2,hash_train_x3,hash_train_x4],axis=1)\n",
    "        test=pd.concat([hash_test_x1,hash_test_x2,hash_test_x3,hash_test_x4],axis=1)\n",
    "        \n",
    "        ap.passive_aggressive(train,train_y2,test,test_y2,stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[26483     0]\n",
      " [    0 47931]]\n",
      "test accuracy: 1.0\n",
      "test confusion matrix\n",
      "[[ 6750     0]\n",
      " [    0 11854]]\n"
     ]
    }
   ],
   "source": [
    "mult_hash4_pac(x1=route2_x.domain,x2=route2_x.content,x3=route2_x.authors,y=route2_y,x4=route2_x.title,stop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Splitting\n",
      "Final size of train/test : 74414 / 18604\n",
      "Early Stopping is being used\n",
      "train accuracy: 1.0\n",
      "train confusion matrix\n",
      "[[26483     0]\n",
      " [    0 47931]]\n",
      "test accuracy: 1.0\n",
      "test confusion matrix\n",
      "[[ 6750     0]\n",
      " [    0 11854]]\n"
     ]
    }
   ],
   "source": [
    "mult_hash4_pac(x1=route2_x.domain,x2=route2_x.content,x3=route2_x.authors,y=route2_y,x4=route2_x.title,stop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
